{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M-CcfOaSi2C4",
        "outputId": "a4c00bfe-2970-49ba-f685-7c66ca3cd4cb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.50.3)\n",
            "Requirement already satisfied: deepface in /usr/local/lib/python3.11/dist-packages (0.0.93)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (3.4.1)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.11/dist-packages (4.11.0.86)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.30.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: pandas>=0.23.4 in /usr/local/lib/python3.11/dist-packages (from deepface) (2.2.2)\n",
            "Requirement already satisfied: gdown>=3.10.1 in /usr/local/lib/python3.11/dist-packages (from deepface) (5.2.0)\n",
            "Requirement already satisfied: Pillow>=5.2.0 in /usr/local/lib/python3.11/dist-packages (from deepface) (11.1.0)\n",
            "Requirement already satisfied: tensorflow>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from deepface) (2.18.0)\n",
            "Requirement already satisfied: keras>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from deepface) (3.8.0)\n",
            "Requirement already satisfied: Flask>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from deepface) (3.1.0)\n",
            "Requirement already satisfied: flask-cors>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from deepface) (5.0.1)\n",
            "Requirement already satisfied: mtcnn>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from deepface) (1.0.0)\n",
            "Requirement already satisfied: retina-face>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from deepface) (0.0.17)\n",
            "Requirement already satisfied: fire>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from deepface) (0.7.0)\n",
            "Requirement already satisfied: gunicorn>=20.1.0 in /usr/local/lib/python3.11/dist-packages (from deepface) (23.0.0)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (2.6.0+cu124)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.14.1)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.11/dist-packages (from fire>=0.4.0->deepface) (3.0.1)\n",
            "Requirement already satisfied: Werkzeug>=3.1 in /usr/local/lib/python3.11/dist-packages (from Flask>=1.1.2->deepface) (3.1.3)\n",
            "Requirement already satisfied: Jinja2>=3.1.2 in /usr/local/lib/python3.11/dist-packages (from Flask>=1.1.2->deepface) (3.1.6)\n",
            "Requirement already satisfied: itsdangerous>=2.2 in /usr/local/lib/python3.11/dist-packages (from Flask>=1.1.2->deepface) (2.2.0)\n",
            "Requirement already satisfied: click>=8.1.3 in /usr/local/lib/python3.11/dist-packages (from Flask>=1.1.2->deepface) (8.1.8)\n",
            "Requirement already satisfied: blinker>=1.9 in /usr/local/lib/python3.11/dist-packages (from Flask>=1.1.2->deepface) (1.9.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from gdown>=3.10.1->deepface) (4.13.3)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (2025.3.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (4.13.0)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from keras>=2.2.0->deepface) (1.4.0)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=2.2.0->deepface) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=2.2.0->deepface) (0.0.8)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.11/dist-packages (from keras>=2.2.0->deepface) (3.13.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=2.2.0->deepface) (0.14.1)\n",
            "Requirement already satisfied: ml-dtypes in /usr/local/lib/python3.11/dist-packages (from keras>=2.2.0->deepface) (0.4.1)\n",
            "Requirement already satisfied: joblib>=1.4.2 in /usr/local/lib/python3.11/dist-packages (from mtcnn>=0.1.0->deepface) (1.4.2)\n",
            "Requirement already satisfied: lz4>=4.3.3 in /usr/local/lib/python3.11/dist-packages (from mtcnn>=0.1.0->deepface) (4.4.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.23.4->deepface) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.23.4->deepface) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.23.4->deepface) (2025.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=1.9.0->deepface) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=1.9.0->deepface) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=1.9.0->deepface) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=1.9.0->deepface) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=1.9.0->deepface) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=1.9.0->deepface) (3.4.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=1.9.0->deepface) (5.29.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow>=1.9.0->deepface) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=1.9.0->deepface) (1.17.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=1.9.0->deepface) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=1.9.0->deepface) (1.71.0)\n",
            "Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=1.9.0->deepface) (2.18.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=1.9.0->deepface) (0.37.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow>=1.9.0->deepface) (0.45.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from Jinja2>=3.1.2->Flask>=1.1.2->deepface) (3.0.2)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow>=1.9.0->deepface) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow>=1.9.0->deepface) (0.7.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->gdown>=3.10.1->deepface) (2.6)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown>=3.10.1->deepface) (1.7.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=2.2.0->deepface) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=2.2.0->deepface) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=2.2.0->deepface) (0.1.2)\n",
            "Requirement already satisfied: python-docx in /usr/local/lib/python3.11/dist-packages (1.1.2)\n",
            "Requirement already satisfied: PyPDF2 in /usr/local/lib/python3.11/dist-packages (3.0.1)\n",
            "Requirement already satisfied: pytesseract in /usr/local/lib/python3.11/dist-packages (0.3.13)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (11.1.0)\n",
            "Requirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from python-docx) (5.3.1)\n",
            "Requirement already satisfied: typing-extensions>=4.9.0 in /usr/local/lib/python3.11/dist-packages (from python-docx) (4.13.0)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.11/dist-packages (from pytesseract) (24.2)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "tesseract-ocr is already the newest version (4.1.1-2.1build1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 30 not upgraded.\n",
            "Requirement already satisfied: pytesseract in /usr/local/lib/python3.11/dist-packages (0.3.13)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.11/dist-packages (from pytesseract) (24.2)\n",
            "Requirement already satisfied: Pillow>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from pytesseract) (11.1.0)\n",
            "Educational Assistant System\n",
            "===========================\n",
            "1. Initialize the system\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Device set to use cpu\n",
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "System initialized successfully!\n",
            "\n",
            "You can use the following functions:\n",
            "1. assistant.upload_and_process_files() - For grading assignments\n",
            "2. assistant.process_uploaded_image_for_emotions() - For analyzing emotions in a single image\n",
            "3. assistant.monitor_classroom_using_colab_camera() - For live classroom monitoring (30 seconds)\n",
            "4. face_detect() - For live classroom monitoring at real time\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers deepface sentence-transformers opencv-python\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from transformers import pipeline, AutoModelForSequenceClassification, AutoTokenizer\n",
        "import cv2\n",
        "from deepface import DeepFace\n",
        "import torch\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from google.colab.patches import cv2_imshow\n",
        "from google.colab.output import eval_js\n",
        "from google.colab import files\n",
        "from IPython.display import display, Javascript, Image, clear_output\n",
        "from base64 import b64decode, b64encode\n",
        "import io\n",
        "import threading\n",
        "import time\n",
        "import PIL\n",
        "\n",
        "!pip install python-docx PyPDF2 pytesseract Pillow\n",
        "!apt-get install tesseract-ocr\n",
        "!pip install pytesseract\n",
        "\n",
        "import docx\n",
        "import PyPDF2\n",
        "import pytesseract\n",
        "from PIL import Image\n",
        "import html\n",
        "\n",
        "\n",
        "# ======== DOCUMENT GRADING SYSTEM ========\n",
        "\n",
        "class AssignmentGrader:\n",
        "    def __init__(self):\n",
        "\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")\n",
        "        self.model = AutoModelForSequenceClassification.from_pretrained(\"roberta-base\")\n",
        "        self.summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
        "        self.nlp = pipeline(\"text-classification\", model=\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
        "\n",
        "\n",
        "        from sentence_transformers import SentenceTransformer\n",
        "        self.semantic_model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
        "\n",
        "\n",
        "\n",
        "    def extract_text_from_document(self, file_path):\n",
        "        \"\"\"Extract text from various document types including images\"\"\"\n",
        "\n",
        "        file_extension = file_path.split('.')[-1].lower()\n",
        "\n",
        "        try:\n",
        "\n",
        "            if file_extension == 'txt':\n",
        "                with open(file_path, 'r', encoding='utf-8', errors='replace') as file:\n",
        "                    return file.read()\n",
        "\n",
        "\n",
        "            elif file_extension in ['docx', 'doc']:\n",
        "                try:\n",
        "                    doc = docx.Document(file_path)\n",
        "                    full_text = []\n",
        "\n",
        "                    for para in doc.paragraphs:\n",
        "                        full_text.append(para.text)\n",
        "\n",
        "                    for table in doc.tables:\n",
        "                        for row in table.rows:\n",
        "                            for cell in row.cells:\n",
        "                                full_text.append(cell.text)\n",
        "                    return '\\n'.join(full_text)\n",
        "                except Exception as e:\n",
        "                    print(f\"Error parsing Word document: {e}\")\n",
        "                    return \"\"\n",
        "\n",
        "\n",
        "            elif file_extension == 'pdf':\n",
        "                try:\n",
        "                    with open(file_path, 'rb') as file:\n",
        "                        pdf_reader = PyPDF2.PdfReader(file)\n",
        "                        text = []\n",
        "                        for page_num in range(len(pdf_reader.pages)):\n",
        "                            page = pdf_reader.pages[page_num]\n",
        "                            text.append(page.extract_text())\n",
        "                        return '\\n'.join(text)\n",
        "                except Exception as e:\n",
        "                    print(f\"Error parsing PDF: {e}\")\n",
        "                    return \"\"\n",
        "\n",
        "\n",
        "            elif file_extension in ['png', 'jpg', 'jpeg', 'bmp', 'tiff', 'webp']:\n",
        "                try:\n",
        "\n",
        "                    image = Image.open(file_path)\n",
        "\n",
        "                    text = pytesseract.image_to_string(image)\n",
        "                    return text\n",
        "                except Exception as e:\n",
        "                    print(f\"Error extracting text from image: {e}\")\n",
        "                    return \"\"\n",
        "\n",
        "\n",
        "            else:\n",
        "                print(f\"Unsupported file type: {file_extension}\")\n",
        "                print(\"Attempting to read as plain text...\")\n",
        "                try:\n",
        "                    with open(file_path, 'r', encoding='utf-8', errors='replace') as file:\n",
        "                        return file.read()\n",
        "                except:\n",
        "                    print(\"Failed to extract text. Please upload a supported file format.\")\n",
        "                    return \"\"\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing file: {e}\")\n",
        "            return \"\"\n",
        "\n",
        "    def parse_rubric(self, rubric_text):\n",
        "        \"\"\"Parse teacher's rubric/notes into assessment criteria\"\"\"\n",
        "\n",
        "        criteria = {}\n",
        "        lines = rubric_text.strip().split('\\n')\n",
        "        for line in lines:\n",
        "            if ':' in line:\n",
        "                key, value = line.split(':', 1)\n",
        "                criteria[key.strip()] = value.strip()\n",
        "        return criteria\n",
        "\n",
        "    def evaluate_content_against_criteria(self, student_text, criteria):\n",
        "        \"\"\"Compare student content against rubric criteria using semantic similarity\"\"\"\n",
        "        scores = {}\n",
        "        student_embedding = self.semantic_model.encode(student_text)\n",
        "\n",
        "        for criterion_name, criterion_desc in criteria.items():\n",
        "            criterion_embedding = self.semantic_model.encode(criterion_desc)\n",
        "            similarity = cosine_similarity(\n",
        "                [student_embedding],\n",
        "                [criterion_embedding]\n",
        "            )[0][0]\n",
        "            scores[criterion_name] = similarity\n",
        "\n",
        "        return scores\n",
        "\n",
        "    def check_grammar_and_structure(self, text):\n",
        "        \"\"\"Check grammar and structure using pre-trained model\"\"\"\n",
        "\n",
        "        sentences = text.split('.')\n",
        "        errors = []\n",
        "        for sentence in sentences:\n",
        "            if len(sentence.strip()) > 5:\n",
        "                sentiment = self.nlp(sentence)\n",
        "\n",
        "\n",
        "        structure_score = min(1.0, len(text.split('\\n\\n')) / 5)\n",
        "        return {\n",
        "            \"grammar_score\": 0.8,\n",
        "            \"structure_score\": structure_score,\n",
        "            \"errors\": errors\n",
        "        }\n",
        "\n",
        "    def generate_feedback(self, content_scores, grammar_results, student_text):\n",
        "        \"\"\"Generate personalized feedback based on assessment\"\"\"\n",
        "\n",
        "        summary = \"\"\n",
        "        if len(student_text) > 200:\n",
        "            try:\n",
        "\n",
        "                max_length = min(100, len(student_text) // 3)\n",
        "                min_length = min(30, max_length - 10)\n",
        "\n",
        "                if max_length > min_length:\n",
        "                    summary = self.summarizer(\n",
        "                        student_text[:1024],\n",
        "                        max_length=max_length,\n",
        "                        min_length=min_length\n",
        "                    )[0]['summary_text']\n",
        "            except Exception as e:\n",
        "                print(f\"Summarization error: {e}\")\n",
        "                summary = \"\"\n",
        "\n",
        "        summary_intro = f\"Summary of your work: {summary}\\n\\n\" if summary else \"\"\n",
        "\n",
        "\n",
        "        feedback = [summary_intro] if summary_intro else []\n",
        "        feedback.append(\"Strengths:\")\n",
        "\n",
        "\n",
        "        if content_scores:\n",
        "            strengths = sorted(content_scores.items(), key=lambda x: x[1], reverse=True)[:2]\n",
        "            for criterion, score in strengths:\n",
        "                feedback.append(f\"• {criterion}: Your work shows strong understanding in this area.\")\n",
        "        else:\n",
        "            feedback.append(\"• Unable to determine specific strengths due to text extraction issues.\")\n",
        "\n",
        "        feedback.append(\"\\nAreas for improvement:\")\n",
        "\n",
        "        if content_scores:\n",
        "            weaknesses = sorted(content_scores.items(), key=lambda x: x[1])[:2]\n",
        "            for criterion, score in weaknesses:\n",
        "                feedback.append(f\"• {criterion}: Consider enhancing your work by focusing more on this aspect.\")\n",
        "        else:\n",
        "            feedback.append(\"• Unable to determine specific improvement areas due to text extraction issues.\")\n",
        "\n",
        "\n",
        "        if grammar_results[\"grammar_score\"] < 0.7:\n",
        "            feedback.append(\"• Check your grammar and sentence structure for clarity.\")\n",
        "\n",
        "        if grammar_results[\"structure_score\"] < 0.6:\n",
        "            feedback.append(\"• Consider improving paragraph organization for better flow.\")\n",
        "\n",
        "        return \"\\n\".join(feedback)\n",
        "    def calculate_grade(self, content_scores, grammar_results):\n",
        "        \"\"\"Calculate overall grade based on all assessments\"\"\"\n",
        "        content_weight = 0.7\n",
        "        grammar_weight = 0.2\n",
        "        structure_weight = 0.1\n",
        "\n",
        "        avg_content_score = sum(content_scores.values()) / len(content_scores)\n",
        "\n",
        "        final_score = (\n",
        "            avg_content_score * content_weight +\n",
        "            grammar_results[\"grammar_score\"] * grammar_weight +\n",
        "            grammar_results[\"structure_score\"] * structure_weight\n",
        "        )\n",
        "\n",
        "\n",
        "        if final_score >= 0.9:\n",
        "            letter_grade = \"A\"\n",
        "        elif final_score >= 0.8:\n",
        "            letter_grade = \"B\"\n",
        "        elif final_score >= 0.7:\n",
        "            letter_grade = \"C\"\n",
        "        elif final_score >= 0.6:\n",
        "            letter_grade = \"D\"\n",
        "        else:\n",
        "            letter_grade = \"F\"\n",
        "\n",
        "        return {\n",
        "            \"numerical_score\": round(final_score * 100, 1),\n",
        "            \"letter_grade\": letter_grade\n",
        "        }\n",
        "\n",
        "    def grade_assignment(self, student_file, rubric_file):\n",
        "        \"\"\"Main function to grade an assignment\"\"\"\n",
        "        student_text = self.extract_text_from_document(student_file)\n",
        "        rubric_text = self.extract_text_from_document(rubric_file)\n",
        "\n",
        "        criteria = self.parse_rubric(rubric_text)\n",
        "        content_scores = self.evaluate_content_against_criteria(student_text, criteria)\n",
        "        grammar_results = self.check_grammar_and_structure(student_text)\n",
        "\n",
        "        feedback = self.generate_feedback(content_scores, grammar_results, student_text)\n",
        "        grade = self.calculate_grade(content_scores, grammar_results)\n",
        "\n",
        "        return {\n",
        "            \"grade\": grade,\n",
        "            \"feedback\": feedback,\n",
        "            \"content_scores\": content_scores,\n",
        "            \"grammar_results\": grammar_results\n",
        "        }\n",
        "\n",
        "\n",
        "# ======== EMOTION DETECTION SYSTEM ========\n",
        "\n",
        "class EmotionDetector:\n",
        "    def __init__(self):\n",
        "\n",
        "        self.face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
        "\n",
        "\n",
        "        self.emotions = [\"angry\", \"disgust\", \"fear\", \"happy\", \"sad\", \"surprise\", \"neutral\"]\n",
        "\n",
        "    def detect_emotions_from_frame(self, frame):\n",
        "        \"\"\"Detect emotions from a video frame\"\"\"\n",
        "\n",
        "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "        faces = self.face_cascade.detectMultiScale(\n",
        "            gray,\n",
        "            scaleFactor=1.1,\n",
        "            minNeighbors=5,\n",
        "            minSize=(30, 30)\n",
        "        )\n",
        "\n",
        "        results = []\n",
        "        for (x, y, w, h) in faces:\n",
        "            face_region = frame[y:y+h, x:x+w]\n",
        "\n",
        "            try:\n",
        "\n",
        "                analysis = DeepFace.analyze(\n",
        "                    face_region,\n",
        "                    actions=['emotion'],\n",
        "                    enforce_detection=False\n",
        "                )\n",
        "\n",
        "                emotion = analysis[0]['dominant_emotion']\n",
        "                emotion_scores = analysis[0]['emotion']\n",
        "\n",
        "                results.append({\n",
        "                    'position': (x, y, w, h),\n",
        "                    'dominant_emotion': emotion,\n",
        "                    'emotion_scores': emotion_scores\n",
        "                })\n",
        "\n",
        "            except Exception as e:\n",
        "\n",
        "                print(f\"Error in emotion detection: {e}\")\n",
        "                continue\n",
        "\n",
        "        return results\n",
        "\n",
        "    def process_uploaded_image(self, image_path):\n",
        "        \"\"\"Process an uploaded image for emotion detection\"\"\"\n",
        "        frame = cv2.imread(image_path)\n",
        "        if frame is None:\n",
        "            return {\"error\": \"Failed to load image\"}\n",
        "\n",
        "        emotion_results = self.detect_emotions_from_frame(frame)\n",
        "\n",
        "\n",
        "        for result in emotion_results:\n",
        "            x, y, w, h = result['position']\n",
        "            emotion = result['dominant_emotion']\n",
        "\n",
        "            cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
        "            cv2.putText(frame, emotion, (x, y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n",
        "\n",
        "\n",
        "        cv2_imshow(frame)\n",
        "\n",
        "        return emotion_results\n",
        "\n",
        "    def analyze_classroom_emotions(self, frame):\n",
        "        \"\"\"Analyze emotions across the classroom\"\"\"\n",
        "        emotion_results = self.detect_emotions_from_frame(frame)\n",
        "\n",
        "\n",
        "        emotion_counts = {emotion: 0 for emotion in self.emotions}\n",
        "        for result in emotion_results:\n",
        "            emotion_counts[result['dominant_emotion']] += 1\n",
        "\n",
        "\n",
        "        engaged_emotions = ['happy', 'surprise']\n",
        "        disengaged_emotions = ['sad', 'angry', 'disgust']\n",
        "        neutral_emotions = ['neutral', 'fear']\n",
        "\n",
        "        total_faces = len(emotion_results)\n",
        "        if total_faces == 0:\n",
        "            return {\n",
        "                'emotion_distribution': emotion_counts,\n",
        "                'engagement_score': 0,\n",
        "                'dominant_mood': 'unknown'\n",
        "            }\n",
        "\n",
        "        engaged_count = sum(emotion_counts[e] for e in engaged_emotions)\n",
        "        disengaged_count = sum(emotion_counts[e] for e in disengaged_emotions)\n",
        "\n",
        "        engagement_score = (engaged_count - disengaged_count) / total_faces\n",
        "\n",
        "\n",
        "        dominant_mood = max(emotion_counts.items(), key=lambda x: x[1])[0] if emotion_counts else 'unknown'\n",
        "\n",
        "        return {\n",
        "            'emotion_distribution': emotion_counts,\n",
        "            'engagement_score': engagement_score,\n",
        "            'dominant_mood': dominant_mood\n",
        "        }\n",
        "\n",
        "# ======== COLAB CAMERA INTEGRATION ========\n",
        "\n",
        "\n",
        "\n",
        "def take_photo(filename='photo.jpg', quality=0.8):\n",
        "    \"\"\"Use JavaScript to capture an image from the webcam\"\"\"\n",
        "    js = Javascript('''\n",
        "    async function takePhoto(quality) {\n",
        "      const div = document.createElement('div');\n",
        "      const capture = document.createElement('button');\n",
        "      capture.textContent = 'Capture';\n",
        "      div.appendChild(capture);\n",
        "\n",
        "      const video = document.createElement('video');\n",
        "      video.style.display = 'block';\n",
        "      const stream = await navigator.mediaDevices.getUserMedia({video: true});\n",
        "\n",
        "      document.body.appendChild(div);\n",
        "      div.appendChild(video);\n",
        "      video.srcObject = stream;\n",
        "      await video.play();\n",
        "\n",
        "\n",
        "      google.colab.output.setIframeHeight(document.documentElement.scrollHeight, true);\n",
        "\n",
        "\n",
        "      await new Promise((resolve) => {\n",
        "        capture.onclick = resolve;\n",
        "      });\n",
        "\n",
        "      const canvas = document.createElement('canvas');\n",
        "      canvas.width = video.videoWidth;\n",
        "      canvas.height = video.videoHeight;\n",
        "      canvas.getContext('2d').drawImage(video, 0, 0);\n",
        "      stream.getVideoTracks()[0].stop();\n",
        "      div.remove();\n",
        "      return canvas.toDataURL('image/jpeg', quality);\n",
        "    }\n",
        "    ''')\n",
        "    display(js)\n",
        "    data = eval_js('takePhoto({})'.format(quality))\n",
        "    binary = b64decode(data.split(',')[1])\n",
        "    with open(filename, 'wb') as f:\n",
        "        f.write(binary)\n",
        "    return filename\n",
        "\n",
        "\n",
        "def get_webcam_frame():\n",
        "    \"\"\"Capture a frame from the webcam using Colab's JavaScript interface\"\"\"\n",
        "    from IPython.display import display, Javascript\n",
        "    from google.colab.output import eval_js\n",
        "\n",
        "    js_code = '''\n",
        "    async function captureFrame() {\n",
        "      const video = document.createElement('video');\n",
        "      video.style.display = 'none';\n",
        "      const stream = await navigator.mediaDevices.getUserMedia({video: true});\n",
        "      document.body.appendChild(video);\n",
        "      video.srcObject = stream;\n",
        "      await video.play();\n",
        "\n",
        "      // Wait a bit for camera to initialize\n",
        "      await new Promise(r => setTimeout(r, 1000));\n",
        "\n",
        "      const canvas = document.createElement('canvas');\n",
        "      canvas.width = video.videoWidth;\n",
        "      canvas.height = video.videoHeight;\n",
        "      canvas.getContext('2d').drawImage(video, 0, 0);\n",
        "      stream.getVideoTracks()[0].stop();\n",
        "      video.remove();\n",
        "\n",
        "      return canvas.toDataURL('image/jpeg', 0.8);\n",
        "    }\n",
        "    '''\n",
        "\n",
        "    display(Javascript(js_code))\n",
        "    try:\n",
        "        data = eval_js('captureFrame()')\n",
        "        img = js_to_image(data)\n",
        "        return img\n",
        "    except Exception as e:\n",
        "        print(f\"Error capturing frame: {e}\")\n",
        "        return None\n",
        "\n",
        "# ======== REAl TIME EMOTION DETECTION ========\n",
        "\n",
        "face_cascade = cv2.CascadeClassifier(cv2.samples.findFile(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml'))\n",
        "\n",
        "def video_stream():\n",
        "  js = Javascript('''\n",
        "    var video;\n",
        "    var div = null;\n",
        "    var stream;\n",
        "    var captureCanvas;\n",
        "    var imgElement;\n",
        "    var labelElement;\n",
        "\n",
        "    var pendingResolve = null;\n",
        "    var shutdown = false;\n",
        "\n",
        "    function removeDom() {\n",
        "       stream.getVideoTracks()[0].stop();\n",
        "       video.remove();\n",
        "       div.remove();\n",
        "       video = null;\n",
        "       div = null;\n",
        "       stream = null;\n",
        "       imgElement = null;\n",
        "       captureCanvas = null;\n",
        "       labelElement = null;\n",
        "    }\n",
        "\n",
        "    function onAnimationFrame() {\n",
        "      if (!shutdown) {\n",
        "        window.requestAnimationFrame(onAnimationFrame);\n",
        "      }\n",
        "      if (pendingResolve) {\n",
        "        var result = \"\";\n",
        "        if (!shutdown) {\n",
        "          captureCanvas.getContext('2d').drawImage(video, 0, 0, 640, 480);\n",
        "          result = captureCanvas.toDataURL('image/jpeg', 0.8)\n",
        "        }\n",
        "        var lp = pendingResolve;\n",
        "        pendingResolve = null;\n",
        "        lp(result);\n",
        "      }\n",
        "    }\n",
        "\n",
        "    async function createDom() {\n",
        "      if (div !== null) {\n",
        "        return stream;\n",
        "      }\n",
        "\n",
        "      div = document.createElement('div');\n",
        "      div.style.border = '2px solid black';\n",
        "      div.style.padding = '3px';\n",
        "      div.style.width = '100%';\n",
        "      div.style.maxWidth = '600px';\n",
        "      document.body.appendChild(div);\n",
        "\n",
        "      const modelOut = document.createElement('div');\n",
        "      modelOut.innerHTML = \"<span>Status:</span>\";\n",
        "      labelElement = document.createElement('span');\n",
        "      labelElement.innerText = 'No data';\n",
        "      labelElement.style.fontWeight = 'bold';\n",
        "      modelOut.appendChild(labelElement);\n",
        "      div.appendChild(modelOut);\n",
        "\n",
        "      video = document.createElement('video');\n",
        "      video.style.display = 'block';\n",
        "      video.width = div.clientWidth - 6;\n",
        "      video.setAttribute('playsinline', '');\n",
        "      video.onclick = () => { shutdown = true; };\n",
        "      stream = await navigator.mediaDevices.getUserMedia(\n",
        "          {video: { facingMode: \"environment\"}});\n",
        "      div.appendChild(video);\n",
        "\n",
        "      imgElement = document.createElement('img');\n",
        "      imgElement.style.position = 'absolute';\n",
        "      imgElement.style.zIndex = 1;\n",
        "      imgElement.onclick = () => { shutdown = true; };\n",
        "      div.appendChild(imgElement);\n",
        "\n",
        "      const instruction = document.createElement('div');\n",
        "      instruction.innerHTML =\n",
        "          '<span style=\"color: red; font-weight: bold;\">' +\n",
        "          'When finished, click here or on the video to stop this demo</span>';\n",
        "      div.appendChild(instruction);\n",
        "      instruction.onclick = () => { shutdown = true; };\n",
        "\n",
        "      video.srcObject = stream;\n",
        "      await video.play();\n",
        "\n",
        "      captureCanvas = document.createElement('canvas');\n",
        "      captureCanvas.width = 640;\n",
        "      captureCanvas.height = 480;\n",
        "      window.requestAnimationFrame(onAnimationFrame);\n",
        "\n",
        "      return stream;\n",
        "    }\n",
        "    async function stream_frame(label, imgData) {\n",
        "      if (shutdown) {\n",
        "        removeDom();\n",
        "        shutdown = false;\n",
        "        return '';\n",
        "      }\n",
        "\n",
        "      var preCreate = Date.now();\n",
        "      stream = await createDom();\n",
        "\n",
        "      var preShow = Date.now();\n",
        "      if (label != \"\") {\n",
        "        labelElement.innerHTML = label;\n",
        "      }\n",
        "\n",
        "      if (imgData != \"\") {\n",
        "        var videoRect = video.getClientRects()[0];\n",
        "        imgElement.style.top = videoRect.top + \"px\";\n",
        "        imgElement.style.left = videoRect.left + \"px\";\n",
        "        imgElement.style.width = videoRect.width + \"px\";\n",
        "        imgElement.style.height = videoRect.height + \"px\";\n",
        "        imgElement.src = imgData;\n",
        "      }\n",
        "\n",
        "      var preCapture = Date.now();\n",
        "      var result = await new Promise(function(resolve, reject) {\n",
        "        pendingResolve = resolve;\n",
        "      });\n",
        "      shutdown = false;\n",
        "\n",
        "      return {'create': preShow - preCreate,\n",
        "              'show': preCapture - preShow,\n",
        "              'capture': Date.now() - preCapture,\n",
        "              'img': result};\n",
        "    }\n",
        "    ''')\n",
        "\n",
        "  display(js)\n",
        "\n",
        "def video_frame(label, bbox):\n",
        "  data = eval_js('stream_frame(\"{}\", \"{}\")'.format(label, bbox))\n",
        "  return data\n",
        "\n",
        "\n",
        "\n",
        "def js_to_image(js_reply):\n",
        "  \"\"\"\n",
        "  Params:\n",
        "          js_reply: JavaScript object containing image from webcam\n",
        "  Returns:\n",
        "          img: OpenCV BGR image\n",
        "  \"\"\"\n",
        "\n",
        "  image_bytes = b64decode(js_reply.split(',')[1])\n",
        "\n",
        "  jpg_as_np = np.frombuffer(image_bytes, dtype=np.uint8)\n",
        "\n",
        "  img = cv2.imdecode(jpg_as_np, flags=1)\n",
        "\n",
        "  return img\n",
        "\n",
        "\n",
        "def bbox_to_bytes(bbox_array):\n",
        "  \"\"\"\n",
        "  Params:\n",
        "          bbox_array: Numpy array (pixels) containing rectangle to overlay on video stream.\n",
        "  Returns:\n",
        "        bytes: Base64 image byte string\n",
        "  \"\"\"\n",
        "\n",
        "  bbox_PIL = PIL.Image.fromarray(bbox_array, 'RGBA')\n",
        "  iobuf = io.BytesIO()\n",
        "\n",
        "  bbox_PIL.save(iobuf, format='png')\n",
        "\n",
        "  bbox_bytes = 'data:image/png;base64,{}'.format((str(b64encode(iobuf.getvalue()), 'utf-8')))\n",
        "\n",
        "  return bbox_bytes\n",
        "\n",
        "\n",
        "def face_detect():\n",
        "  import cv2\n",
        "  from PIL import Image\n",
        "  import numpy as np\n",
        "  import os\n",
        "  from keras.models import load_model\n",
        "  from time import sleep\n",
        "  from keras.preprocessing.image import img_to_array\n",
        "  from keras.preprocessing import image\n",
        "\n",
        "  video_stream()\n",
        "\n",
        "  label_html = 'Capturing...'\n",
        "\n",
        "  bbox = ''\n",
        "  count = 0\n",
        "  face_classifier = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml') # Face Detection\n",
        "  classifier =load_model('/content/model_78.h5')  #Load model\n",
        "  emotion_labels = ['Angry','Disgust','Fear','Happy','Neutral', 'Sad', 'Surprise']\n",
        "\n",
        "  while True:\n",
        "    js_reply = video_frame(label_html, bbox)\n",
        "    if not js_reply:\n",
        "      break\n",
        "\n",
        "\n",
        "    img = js_to_image(js_reply[\"img\"])\n",
        "\n",
        "\n",
        "    bbox_array = np.zeros([480,640,4], dtype=np.uint8)\n",
        "\n",
        "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "\n",
        "    faces = face_cascade.detectMultiScale(gray)\n",
        "\n",
        "    for (x,y,w,h) in faces:\n",
        "      bbox_array = cv2.rectangle(bbox_array,(x,y),(x+w,y+h),(0,255,0),2)\n",
        "      roi_gray = gray[y:y+h,x:x+w]\n",
        "      roi_gray = cv2.resize(roi_gray,(48,48),interpolation=cv2.INTER_AREA)\n",
        "      if np.sum([roi_gray])!=0:\n",
        "        roi = roi_gray.astype('float')/255.0\n",
        "        roi = img_to_array(roi)\n",
        "        roi = np.expand_dims(roi,axis=0)\n",
        "        prediction = classifier.predict(roi)[0]   #Prediction\n",
        "        label=emotion_labels[prediction.argmax()]\n",
        "        label_position = (x,y)\n",
        "        cv2.putText(bbox_array,label,label_position,cv2.FONT_HERSHEY_SIMPLEX,1,(0,255,0),2)   # Text Adding\n",
        "        label_html = 'Capturing...' + label\n",
        "      else:\n",
        "        cv2.putText(bbox_array,'No Faces',(30,80),cv2.FONT_HERSHEY_SIMPLEX,1,(0,255,0),2)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    bbox_array[:,:,3] = (bbox_array.max(axis = 2) > 0 ).astype(int) * 255\n",
        "    bbox_bytes = bbox_to_bytes(bbox_array)\n",
        "    bbox = bbox_bytes\n",
        "\n",
        "\n",
        "# ======== INTEGRATED SYSTEM ========\n",
        "\n",
        "class EducationalAssistant:\n",
        "    def __init__(self):\n",
        "        self.grader = AssignmentGrader()\n",
        "        self.emotion_detector = EmotionDetector()\n",
        "\n",
        "\n",
        "    def process_assignment(self, student_file, rubric_file):\n",
        "        \"\"\"Process a student assignment with grading and feedback\"\"\"\n",
        "        return self.grader.grade_assignment(student_file, rubric_file)\n",
        "\n",
        "    def upload_and_process_files(self):\n",
        "        \"\"\"Upload student assignment and rubric files through Colab interface\"\"\"\n",
        "        print(\"Upload student assignment file (txt, docx, pdf, jpg, png, etc.):\")\n",
        "        print(\"This can be an essay, report, assignment, or even an image containing text.\")\n",
        "        uploaded_student = files.upload()\n",
        "        student_file = list(uploaded_student.keys())[0]\n",
        "\n",
        "        print(\"Upload teacher rubric file (txt, docx, pdf, etc.):\")\n",
        "        print(\"This should contain evaluation criteria, typically in format 'Criterion: Description'\")\n",
        "        uploaded_rubric = files.upload()\n",
        "        rubric_file = list(uploaded_rubric.keys())[0]\n",
        "\n",
        "        return self.process_assignment(student_file, rubric_file)\n",
        "\n",
        "    def process_uploaded_image_for_emotions(self):\n",
        "        \"\"\"Upload and process an image for emotion detection\"\"\"\n",
        "        print(\"Upload classroom image for emotion detection:\")\n",
        "        uploaded_images = files.upload()\n",
        "        image_file = list(uploaded_images.keys())[0]\n",
        "\n",
        "        results = self.emotion_detector.process_uploaded_image(image_file)\n",
        "        emotion_data = self.emotion_detector.analyze_classroom_emotions(cv2.imread(image_file))\n",
        "\n",
        "        classroom_report = self.generate_classroom_report(emotion_data)\n",
        "        print(classroom_report)\n",
        "\n",
        "        return results\n",
        "\n",
        "    def monitor_classroom_using_colab_camera(self, duration=30):\n",
        "        \"\"\"Use Colab's camera for classroom monitoring\"\"\"\n",
        "        print(\"Starting classroom monitoring...\")\n",
        "        print(\"This will capture an image from your webcam every 5 seconds and analyze emotions.\")\n",
        "        print(f\"Monitoring will run for {duration} seconds.\")\n",
        "\n",
        "        end_time = time.time() + duration\n",
        "\n",
        "        while time.time() < end_time:\n",
        "            print(\"Capturing frame...\")\n",
        "            try:\n",
        "\n",
        "                photo_filename = take_photo()\n",
        "                frame = cv2.imread(photo_filename)\n",
        "\n",
        "                if frame is None:\n",
        "                    print(\"Failed to capture frame. Trying again...\")\n",
        "                    continue\n",
        "\n",
        "                emotion_results = self.emotion_detector.detect_emotions_from_frame(frame)\n",
        "\n",
        "\n",
        "                for result in emotion_results:\n",
        "                    x, y, w, h = result['position']\n",
        "                    emotion = result['dominant_emotion']\n",
        "\n",
        "                    cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
        "                    cv2.putText(frame, emotion, (x, y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n",
        "\n",
        "\n",
        "                cv2_imshow(frame)\n",
        "\n",
        "                emotion_data = self.emotion_detector.analyze_classroom_emotions(frame)\n",
        "                report = self.generate_classroom_report(emotion_data)\n",
        "                print(report)\n",
        "\n",
        "\n",
        "                time.sleep(5)\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error in monitoring: {e}\")\n",
        "                time.sleep(1)\n",
        "\n",
        "        print(\"Classroom monitoring completed.\")\n",
        "\n",
        "    def generate_classroom_report(self, emotion_data):\n",
        "        \"\"\"Generate report on classroom emotions\"\"\"\n",
        "        report = [\"## Classroom Emotional Analysis Report\"]\n",
        "\n",
        "        report.append(\"\\n### Emotion Distribution:\")\n",
        "        for emotion, count in emotion_data['emotion_distribution'].items():\n",
        "            report.append(f\"- {emotion.capitalize()}: {count} students\")\n",
        "\n",
        "        report.append(f\"\\n### Engagement Score: {emotion_data['engagement_score']:.2f}\")\n",
        "        report.append(f\"(-1.0 = completely disengaged, 1.0 = fully engaged)\")\n",
        "\n",
        "        report.append(f\"\\n### Dominant Mood: {emotion_data['dominant_mood'].capitalize()}\")\n",
        "\n",
        "        report.append(\"\\n### Recommendations:\")\n",
        "        if emotion_data['engagement_score'] < -0.3:\n",
        "            report.append(\"- Consider an activity change to re-engage students\")\n",
        "            report.append(\"- Check if the material might be too challenging or not challenging enough\")\n",
        "        elif emotion_data['dominant_mood'] in ['sad', 'angry']:\n",
        "            report.append(\"- Students appear distressed - consider checking in or providing a short break\")\n",
        "        elif emotion_data['dominant_mood'] == 'surprise':\n",
        "            report.append(\"- Students appear stimulated - good time for introducing new concepts\")\n",
        "\n",
        "        return \"\\n\".join(report)\n",
        "\n",
        "\n",
        "\n",
        "print(\"Educational Assistant System\")\n",
        "print(\"===========================\")\n",
        "print(\"1. Initialize the system\")\n",
        "assistant = EducationalAssistant()\n",
        "print(\"System initialized successfully!\")\n",
        "\n",
        "print(\"\\nYou can use the following functions:\")\n",
        "print(\"1. assistant.upload_and_process_files() - For grading assignments\")\n",
        "print(\"2. assistant.process_uploaded_image_for_emotions() - For analyzing emotions in a single image\")\n",
        "print(\"3. assistant.monitor_classroom_using_colab_camera() - For live classroom monitoring (30 seconds)\")\n",
        "print(\"4. face_detect() - For live classroom monitoring at real time\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def run_interactive_menu():\n",
        "    while True:\n",
        "        print(\"\\n===== Educational Assistant Menu =====\")\n",
        "        print(\"1. Grade assignments\")\n",
        "        print(\"2. Analyze emotions in uploaded image\")\n",
        "        print(\"3. Live classroom monitoring (30 seconds)\")\n",
        "        print(\"4. Exit\")\n",
        "\n",
        "        choice = input(\"Enter your choice (1-4): \")\n",
        "\n",
        "        if choice == '1':\n",
        "            result = assistant.upload_and_process_files()\n",
        "            print(\"\\n=== Grading Results ===\")\n",
        "            print(f\"Grade: {result['grade']['letter_grade']} ({result['grade']['numerical_score']}%)\")\n",
        "            print(\"\\nFeedback:\")\n",
        "            print(result['feedback'])\n",
        "\n",
        "        elif choice == '2':\n",
        "            assistant.process_uploaded_image_for_emotions()\n",
        "\n",
        "        elif choice == '3':\n",
        "            duration = 30\n",
        "            try:\n",
        "                duration_input = input(\"Enter monitoring duration in seconds (default: 30): \")\n",
        "                if duration_input.strip():\n",
        "                    duration = int(duration_input)\n",
        "            except ValueError:\n",
        "                print(\"Invalid input, using default 30 seconds\")\n",
        "\n",
        "            assistant.monitor_classroom_using_colab_camera(duration=duration)\n",
        "\n",
        "        elif choice == '4':\n",
        "            print(\"Exiting Educational Assistant. Goodbye!\")\n",
        "            break\n",
        "\n",
        "        else:\n",
        "            print(\"Invalid choice. Please enter a number between 1 and 4.\")\n",
        "\n",
        "# Run the interactive menu\n",
        "run_interactive_menu()"
      ],
      "metadata": {
        "id": "dr6G1jlDGXYM",
        "outputId": "bc8003f1-b38f-4112-ae98-6d37500b85a2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 721
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===== Educational Assistant Menu =====\n",
            "1. Grade assignments\n",
            "2. Analyze emotions in uploaded image\n",
            "3. Live classroom monitoring (30 seconds)\n",
            "4. Exit\n",
            "Enter your choice (1-4): 1\n",
            "Upload student assignment file (txt, docx, pdf, jpg, png, etc.):\n",
            "This can be an essay, report, assignment, or even an image containing text.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-4eceb2ce-91b3-4397-90bc-e940fc068d4c\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-4eceb2ce-91b3-4397-90bc-e940fc068d4c\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving Title.docx to Title.docx\n",
            "Upload teacher rubric file (txt, docx, pdf, etc.):\n",
            "This should contain evaluation criteria, typically in format 'Criterion: Description'\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-b0ccc54e-2b04-4bcd-82ea-905903264fcd\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-b0ccc54e-2b04-4bcd-82ea-905903264fcd\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving Title.docx to Title (1).docx\n",
            "\n",
            "=== Grading Results ===\n",
            "Grade: C (73.0%)\n",
            "\n",
            "Feedback:\n",
            "Summary of your work: Computers have become an integral part of our daily lives, revolutionizing the way we work, communicate, learn, and entertain ourselves. From personal use to industrial applications, computers have dramatically changed the face of the modern world.\n",
            "\n",
            "\n",
            "Strengths:\n",
            "• Title: Your work shows strong understanding in this area.\n",
            "\n",
            "Areas for improvement:\n",
            "• Title: Consider enhancing your work by focusing more on this aspect.\n",
            "• Consider improving paragraph organization for better flow.\n",
            "\n",
            "===== Educational Assistant Menu =====\n",
            "1. Grade assignments\n",
            "2. Analyze emotions in uploaded image\n",
            "3. Live classroom monitoring (30 seconds)\n",
            "4. Exit\n",
            "Enter your choice (1-4): 4\n",
            "Exiting Educational Assistant. Goodbye!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "face_detect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 443
        },
        "id": "SErzSL-K0FvI",
        "outputId": "a68e0e22-33b4-4306-fecd-7288ee838244"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    var video;\n",
              "    var div = null;\n",
              "    var stream;\n",
              "    var captureCanvas;\n",
              "    var imgElement;\n",
              "    var labelElement;\n",
              "\n",
              "    var pendingResolve = null;\n",
              "    var shutdown = false;\n",
              "\n",
              "    function removeDom() {\n",
              "       stream.getVideoTracks()[0].stop();\n",
              "       video.remove();\n",
              "       div.remove();\n",
              "       video = null;\n",
              "       div = null;\n",
              "       stream = null;\n",
              "       imgElement = null;\n",
              "       captureCanvas = null;\n",
              "       labelElement = null;\n",
              "    }\n",
              "\n",
              "    function onAnimationFrame() {\n",
              "      if (!shutdown) {\n",
              "        window.requestAnimationFrame(onAnimationFrame);\n",
              "      }\n",
              "      if (pendingResolve) {\n",
              "        var result = \"\";\n",
              "        if (!shutdown) {\n",
              "          captureCanvas.getContext('2d').drawImage(video, 0, 0, 640, 480);\n",
              "          result = captureCanvas.toDataURL('image/jpeg', 0.8)\n",
              "        }\n",
              "        var lp = pendingResolve;\n",
              "        pendingResolve = null;\n",
              "        lp(result);\n",
              "      }\n",
              "    }\n",
              "\n",
              "    async function createDom() {\n",
              "      if (div !== null) {\n",
              "        return stream;\n",
              "      }\n",
              "\n",
              "      div = document.createElement('div');\n",
              "      div.style.border = '2px solid black';\n",
              "      div.style.padding = '3px';\n",
              "      div.style.width = '100%';\n",
              "      div.style.maxWidth = '600px';\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const modelOut = document.createElement('div');\n",
              "      modelOut.innerHTML = \"<span>Status:</span>\";\n",
              "      labelElement = document.createElement('span');\n",
              "      labelElement.innerText = 'No data';\n",
              "      labelElement.style.fontWeight = 'bold';\n",
              "      modelOut.appendChild(labelElement);\n",
              "      div.appendChild(modelOut);\n",
              "\n",
              "      video = document.createElement('video');\n",
              "      video.style.display = 'block';\n",
              "      video.width = div.clientWidth - 6;\n",
              "      video.setAttribute('playsinline', '');\n",
              "      video.onclick = () => { shutdown = true; };\n",
              "      stream = await navigator.mediaDevices.getUserMedia(\n",
              "          {video: { facingMode: \"environment\"}});\n",
              "      div.appendChild(video);\n",
              "\n",
              "      imgElement = document.createElement('img');\n",
              "      imgElement.style.position = 'absolute';\n",
              "      imgElement.style.zIndex = 1;\n",
              "      imgElement.onclick = () => { shutdown = true; };\n",
              "      div.appendChild(imgElement);\n",
              "\n",
              "      const instruction = document.createElement('div');\n",
              "      instruction.innerHTML =\n",
              "          '<span style=\"color: red; font-weight: bold;\">' +\n",
              "          'When finished, click here or on the video to stop this demo</span>';\n",
              "      div.appendChild(instruction);\n",
              "      instruction.onclick = () => { shutdown = true; };\n",
              "\n",
              "      video.srcObject = stream;\n",
              "      await video.play();\n",
              "\n",
              "      captureCanvas = document.createElement('canvas');\n",
              "      captureCanvas.width = 640; //video.videoWidth;\n",
              "      captureCanvas.height = 480; //video.videoHeight;\n",
              "      window.requestAnimationFrame(onAnimationFrame);\n",
              "\n",
              "      return stream;\n",
              "    }\n",
              "    async function stream_frame(label, imgData) {\n",
              "      if (shutdown) {\n",
              "        removeDom();\n",
              "        shutdown = false;\n",
              "        return '';\n",
              "      }\n",
              "\n",
              "      var preCreate = Date.now();\n",
              "      stream = await createDom();\n",
              "\n",
              "      var preShow = Date.now();\n",
              "      if (label != \"\") {\n",
              "        labelElement.innerHTML = label;\n",
              "      }\n",
              "\n",
              "      if (imgData != \"\") {\n",
              "        var videoRect = video.getClientRects()[0];\n",
              "        imgElement.style.top = videoRect.top + \"px\";\n",
              "        imgElement.style.left = videoRect.left + \"px\";\n",
              "        imgElement.style.width = videoRect.width + \"px\";\n",
              "        imgElement.style.height = videoRect.height + \"px\";\n",
              "        imgElement.src = imgData;\n",
              "      }\n",
              "\n",
              "      var preCapture = Date.now();\n",
              "      var result = await new Promise(function(resolve, reject) {\n",
              "        pendingResolve = resolve;\n",
              "      });\n",
              "      shutdown = false;\n",
              "\n",
              "      return {'create': preShow - preCreate,\n",
              "              'show': preCapture - preShow,\n",
              "              'capture': Date.now() - preCapture,\n",
              "              'img': result};\n",
              "    }\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] Unable to synchronously open file (unable to open file: name = '/content/model_by_nishant.h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-060a14563fab>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mface_detect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-16-f3f010fc2ebf>\u001b[0m in \u001b[0;36mface_detect\u001b[0;34m()\u001b[0m\n\u001b[1;32m    653\u001b[0m   \u001b[0mcount\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    654\u001b[0m   \u001b[0mface_classifier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCascadeClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhaarcascades\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'haarcascade_frontalface_default.xml'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Face Detection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 655\u001b[0;31m   \u001b[0mclassifier\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/model_by_nishant.h5'\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m#Load model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    656\u001b[0m   \u001b[0memotion_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'Angry'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Disgust'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Fear'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Happy'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Neutral'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Sad'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Surprise'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    657\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/saving/saving_api.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile, safe_mode)\u001b[0m\n\u001b[1;32m    194\u001b[0m         )\n\u001b[1;32m    195\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".h5\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\".hdf5\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m         return legacy_h5_format.load_model_from_hdf5(\n\u001b[0m\u001b[1;32m    197\u001b[0m             \u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/legacy/saving/legacy_h5_format.py\u001b[0m in \u001b[0;36mload_model_from_hdf5\u001b[0;34m(filepath, custom_objects, compile)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0mopened_new_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh5py\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mopened_new_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5py\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"r\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfilepath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode, driver, libver, userblock_size, swmr, rdcc_nslots, rdcc_nbytes, rdcc_w0, track_order, fs_strategy, fs_persist, fs_threshold, fs_page_size, page_buf_size, min_meta_keep, min_raw_keep, locking, alignment_threshold, alignment_interval, meta_block_size, **kwds)\u001b[0m\n\u001b[1;32m    562\u001b[0m                                  \u001b[0mfs_persist\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfs_persist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfs_threshold\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfs_threshold\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m                                  fs_page_size=fs_page_size)\n\u001b[0;32m--> 564\u001b[0;31m                 \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_fid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muserblock_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfcpl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mswmr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mswmr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    565\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    566\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlibver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36mmake_fid\u001b[0;34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[0m\n\u001b[1;32m    236\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mswmr\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mswmr_support\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m             \u001b[0mflags\u001b[0m \u001b[0;34m|=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_SWMR_READ\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 238\u001b[0;31m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    239\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'r+'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_RDWR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mh5py/h5f.pyx\u001b[0m in \u001b[0;36mh5py.h5f.open\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] Unable to synchronously open file (unable to open file: name = '/content/model_by_nishant.h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)"
          ]
        }
      ]
    }
  ]
}